{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da29539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== DEPENDENCIES ====================\n",
    "# Install required packages (uncomment if needed)\n",
    "# !pip install tensorflow>=2.13.0\n",
    "# !pip install numpy pandas scikit-learn\n",
    "# !pip install matplotlib seaborn\n",
    "# !pip install fairlearn\n",
    "\n",
    "\"\"\"\n",
    "Required Dependencies:\n",
    "- tensorflow >= 2.13.0 (with GPU support recommended)\n",
    "- numpy >= 1.23.0\n",
    "- pandas >= 1.5.0\n",
    "- scikit-learn >= 1.2.0\n",
    "- matplotlib >= 3.6.0\n",
    "- seaborn >= 0.12.0\n",
    "- fairlearn >= 0.8.0\n",
    "\n",
    "Hardware Requirements:\n",
    "- GPU with CUDA support (recommended for training)\n",
    "- Minimum 16GB RAM\n",
    "- Distributed training across 2+ GPUs supported via tf.distribute.MirroredStrategy\n",
    "\"\"\"\n",
    "\n",
    "print(\"✅ All dependencies loaded successfully\")\n",
    "print(f\"TensorFlow version: {__import__('tensorflow').__version__}\")\n",
    "print(f\"NumPy version: {__import__('numpy').__version__}\")\n",
    "print(f\"Pandas version: {__import__('pandas').__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7483ab81",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# complete_integration_with_fairness.py\n",
    "# Single-file training + eval script with fairness CVaR loss integrated.\n",
    "# EarlyStopping + Best-Weights Checkpointing + EPOCHS=25\n",
    "\n",
    "# ====================== CONFIG ======================\n",
    "DATASET_ROOT = \"/kaggle/input/input-folder/dataset\"\n",
    "PREPROC_DIR  = \"Celeb-DF Preprocessed\"\n",
    "IMAGE_ROOT   = f\"{DATASET_ROOT}/{PREPROC_DIR}/train\"\n",
    "\n",
    "CSV_PATH          = \"/kaggle/input/input-folder/labels.csv\"\n",
    "CSV_REL_PATH_COL  = \"relative_path\"\n",
    "CSV_LABEL_COL     = \"label\"\n",
    "CSV_AGE_COL       = \"age_group\"\n",
    "CSV_GENDER_COL    = \"gender\"\n",
    "CSV_SKIN_COL      = \"skin_tone\"\n",
    "\n",
    "IMG_SIZE    = (128, 128)\n",
    "LATENT_DIM  = 128\n",
    "EPOCHS      = 30         # <— requested\n",
    "BATCH_SIZE  = 64\n",
    "\n",
    "VAL_SPLIT   = 0.20\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "AGE_NUM_CLASSES    = 3\n",
    "GENDER_NUM_CLASSES = 2\n",
    "SKIN_NUM_CLASSES   = 4\n",
    "\n",
    "USE_MIXED_PRECISION = True\n",
    "MAX_SHUFFLE_BUFFER  = 12000\n",
    "\n",
    "# Plot behavior (requested earlier): show plots by default, don't save\n",
    "SAVE_PLOTS = False\n",
    "\n",
    "# Reconstruction loss components weights\n",
    "RECON_IMG_WEIGHT  = 1.0\n",
    "RECON_FRFT_WEIGHT = 1.0\n",
    "\n",
    "# --- ADD: real/fake supervision + schedules ---\n",
    "REALFAKE_LOSS_WEIGHT = 1.5\n",
    "FAIRNESS_WARMUP_EPOCHS = 3\n",
    "\n",
    "# --- Early stopping on custom loop ---\n",
    "ES_PATIENCE = 7                    # epochs\n",
    "CKPT_DIR = \"/kaggle/working/ae_exports/best\"   # best weights location\n",
    "FINAL_SAVE_DIR = \"/kaggle/working/ae_exports\"  # final save dir\n",
    "\n",
    "import os, re, time, math, numpy as np, pandas as pd, tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.data import AUTOTUNE\n",
    "from tensorflow.keras.layers import (Input, Dense, Flatten, Reshape, Dropout,\n",
    "                                     Conv2D, MaxPooling2D, UpSampling2D, LayerNormalization,\n",
    "                                     LeakyReLU, GaussianNoise)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import AdamW, Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy, BinaryCrossentropy\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve, precision_recall_curve, accuracy_score, confusion_matrix\n",
    "from fairlearn.metrics import (\n",
    "    MetricFrame,\n",
    "    selection_rate,\n",
    "    false_positive_rate,\n",
    "    false_negative_rate,\n",
    "    equalized_odds_difference,\n",
    "    demographic_parity_difference,\n",
    ")\n",
    "PREFETCH_BUFSIZE = AUTOTUNE\n",
    "\n",
    "# ============== GPU OPTS ===================\n",
    "tf.config.set_soft_device_placement(True)\n",
    "if USE_MIXED_PRECISION:\n",
    "    mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        try: tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        except: pass\n",
    "    print(\"GPUs:\", gpus)\n",
    "else:\n",
    "    print(\"No GPU; running on CPU.\")\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(\"Replicas in sync:\", strategy.num_replicas_in_sync)\n",
    "GLOBAL_BATCH_SIZE = BATCH_SIZE * strategy.num_replicas_in_sync\n",
    "\n",
    "# --- ADD: Global epoch tracker for warmups used inside steps ---\n",
    "CURRENT_EPOCH_TF = tf.Variable(1.0, dtype=tf.float32, trainable=False)\n",
    "\n",
    "# ================== FRFT (Order=3) =======================\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class FRFTPrecomp:\n",
    "    def __init__(self, H, W, alphas_row=None, alphas_col=None, order=5):\n",
    "        self.H, self.W = H, W\n",
    "\n",
    "        # Handle old-style alpha lists\n",
    "        if alphas_row is not None and isinstance(alphas_row, (list, tuple)):\n",
    "            # Use the middle or last alpha from list as main FRFT order\n",
    "            self.alpha = float(np.mean(alphas_row))\n",
    "        else:\n",
    "            # Fall back to order parameter\n",
    "            self.alpha = np.pi * order / 4.0\n",
    "\n",
    "        self.order = order\n",
    "\n",
    "\n",
    "    def frft2d_vectorized(self, x):\n",
    "        \"\"\"Fully parallel 2D FRFT (chirp–FFT–chirp), GPU-friendly.\"\"\"\n",
    "        H, W = self.H, self.W\n",
    "        alpha = tf.constant(self.alpha, tf.float32)\n",
    "    \n",
    "        # Compute required trig values\n",
    "        tan_a2 = tf.tan(alpha / 2.0)\n",
    "        sin_a = tf.sin(alpha)\n",
    "    \n",
    "        # Generate coordinate grids\n",
    "        h = tf.cast(tf.range(H), tf.float32) - H / 2.0\n",
    "        w = tf.cast(tf.range(W), tf.float32) - W / 2.0\n",
    "    \n",
    "        # Precompute broadcastable chirps (complex64)\n",
    "        jpi = tf.complex(0.0, -np.pi)\n",
    "        chirp_h = tf.exp(jpi * tf.cast((h ** 2) * tan_a2 / H, tf.complex64))\n",
    "        chirp_w = tf.exp(jpi * tf.cast((w ** 2) * tan_a2 / W, tf.complex64))\n",
    "    \n",
    "        # Broadcast over batch\n",
    "        x = tf.cast(x, tf.complex64)\n",
    "        x = x * tf.reshape(chirp_h, [1, H, 1]) * tf.reshape(chirp_w, [1, 1, W])\n",
    "    \n",
    "        # 2D FFT (parallelized)\n",
    "        X = tf.signal.fft2d(x)\n",
    "    \n",
    "        # Post-chirp (cast floats to complex64!)\n",
    "        chirp2_h = tf.exp(jpi * tf.cast((h ** 2) * tan_a2 / H, tf.complex64))\n",
    "        chirp2_w = tf.exp(jpi * tf.cast((w ** 2) * tan_a2 / W, tf.complex64))\n",
    "        X = X * tf.reshape(chirp2_h, [1, H, 1]) * tf.reshape(chirp2_w, [1, 1, W])\n",
    "    \n",
    "        # Global phase & scale correction\n",
    "        phase = tf.exp(tf.complex(0.0, -alpha * np.pi / 4.0))\n",
    "        X = X * phase / tf.sqrt(tf.cast(tf.abs(sin_a), tf.complex64))\n",
    "    \n",
    "        return X\n",
    "\n",
    "    def frft2_grid_mag01(self, img_hw1):\n",
    "        \"\"\"Compute FRFT magnitude + log normalization.\"\"\"\n",
    "        x = tf.cast(img_hw1[..., 0], tf.complex64)\n",
    "        X = self.frft2d_vectorized(x)\n",
    "        mag = tf.abs(X)\n",
    "        mag = tf.math.log1p(mag)\n",
    "        mean = tf.reduce_mean(mag, axis=[0, 1, 2], keepdims=True)\n",
    "        std = tf.math.reduce_std(mag, axis=[0, 1, 2], keepdims=True)\n",
    "        mag_std = (mag - mean) / (std + 1e-6)\n",
    "        return tf.expand_dims(tf.cast(mag_std, tf.float32), -1)\n",
    "\n",
    "# ============== Gradient Reversal ===================\n",
    "@tf.custom_gradient\n",
    "def grad_reverse(x, lam):\n",
    "    lam = tf.cast(lam, x.dtype)\n",
    "    def grad(dy): return -dy * lam, None\n",
    "    return tf.identity(x), grad\n",
    "\n",
    "# ==================== MODELS ========================\n",
    "def build_shared_autoencoder(image_size=(128,128,1), latent_dim=16, input_noise_std=0.05, input_dropout=0.05, leaky_alpha=0.1):\n",
    "    h,w,c = image_size\n",
    "    enc_in = Input(shape=(h,w,c))\n",
    "    x = GaussianNoise(input_noise_std)(enc_in)\n",
    "    x = Dropout(input_dropout)(x)\n",
    "    x = Conv2D(32,(3,3),padding='same',kernel_initializer='he_normal')(x); x = LeakyReLU(leaky_alpha)(x); x = MaxPooling2D((2,2),padding='same')(x)\n",
    "    x = Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal')(x); x = LeakyReLU(leaky_alpha)(x); x = MaxPooling2D((2,2),padding='same')(x)\n",
    "    x = Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal')(x); x = LeakyReLU(leaky_alpha)(x); x = MaxPooling2D((2,2),padding='same')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = LayerNormalization(epsilon=1e-5)(x)\n",
    "    z = Dense(latent_dim, activation=None , dtype='float32',\n",
    "              kernel_initializer='he_normal')(x)\n",
    "    encoder = Model(enc_in, z, name='encoder')\n",
    "\n",
    "    dec_in = Input(shape=(latent_dim,), dtype='float32')\n",
    "    x = Dense((h//8)*(w//8)*128, activation=None, kernel_initializer='he_normal')(dec_in); x = LeakyReLU(leaky_alpha)(x); x = Reshape((h//8, w//8, 128))(x)\n",
    "    x = Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal')(x); x = LeakyReLU(leaky_alpha)(x); x = UpSampling2D((2,2))(x)\n",
    "    x = Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal')(x); x = LeakyReLU(leaky_alpha)(x); x = UpSampling2D((2,2))(x)\n",
    "    x = Conv2D(32,(3,3),padding='same',kernel_initializer='he_normal')(x); x = LeakyReLU(leaky_alpha)(x); x = UpSampling2D((2,2))(x)\n",
    "    out = Conv2D(c,(3,3),activation=None,padding='same', dtype='float32')(x)\n",
    "    decoder = Model(dec_in, out, name='decoder')\n",
    "    return encoder, decoder\n",
    "\n",
    "def build_classifier(latent_dim, leaky_alpha=0.1):\n",
    "    inp = Input(shape=(latent_dim,), dtype='float32')\n",
    "    x = LayerNormalization(epsilon=1e-5)(inp)\n",
    "    x = Dense(max(16, latent_dim//2), activation=None, kernel_initializer='he_normal',\n",
    "              kernel_constraint=MaxNorm(3.0))(x); x = LeakyReLU(leaky_alpha)(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(max(8, latent_dim//4), activation=None, kernel_initializer='he_normal',\n",
    "              kernel_constraint=MaxNorm(3.0))(x); x = LeakyReLU(leaky_alpha)(x)\n",
    "    age_out    = Dense(AGE_NUM_CLASSES,    name='age_output',       dtype='float32')(x)\n",
    "    gender_out = Dense(GENDER_NUM_CLASSES, name='gender_output',    dtype='float32')(x)\n",
    "    skin_out   = Dense(SKIN_NUM_CLASSES,   name='skin_tone_output', dtype='float32')(x)\n",
    "    return Model(inp, [age_out, gender_out, skin_out], name='classifier')\n",
    "\n",
    "def build_realfake_head(latent_dim, leaky_alpha=0.1):\n",
    "    inp = Input(shape=(latent_dim,), dtype='float32')\n",
    "    x = LayerNormalization(epsilon=1e-5)(inp)\n",
    "    x = Dense(max(16, latent_dim//2), activation=None, kernel_initializer='he_normal')(x); x = LeakyReLU(leaky_alpha)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(max(8, latent_dim//4), activation=None, kernel_initializer='he_normal')(x); x = LeakyReLU(leaky_alpha)(x)\n",
    "    out = Dense(1, activation=None, name='real_fake_logit', dtype='float32')(x)\n",
    "    return Model(inp, out, name='realfake_head')\n",
    "\n",
    "# ==================== LOSSES & AUX ========================\n",
    "ce = CategoricalCrossentropy(from_logits=True, label_smoothing=0.05)\n",
    "\n",
    "def latent_loss(z, z_hat, beta=0.5, gamma=0.5, eps=1e-6):\n",
    "    z, z_hat = tf.cast(z, tf.float32), tf.cast(z_hat, tf.float32)\n",
    "    z_t = tf.stop_gradient(z)\n",
    "    diff   = z_hat - z_t\n",
    "    denom  = (tf.reduce_sum(tf.square(z_t),-1)+tf.reduce_sum(tf.square(z_hat),-1)+eps)\n",
    "    rel_l2 = tf.reduce_mean(tf.reduce_sum(tf.square(diff),-1) / denom)\n",
    "    zt_n = tf.math.l2_normalize(z_t, -1)\n",
    "    zh_n = tf.math.l2_normalize(z_hat, -1)\n",
    "    cos_l  = tf.reduce_mean(1.0 - tf.reduce_sum(zt_n*zh_n,-1))\n",
    "    return beta*rel_l2 + gamma*cos_l\n",
    "\n",
    "def logit_norm_penalty(age_logits, gen_logits, skin_logits, alpha=1e-4):\n",
    "    terms = [tf.reduce_mean(tf.square(tf.cast(t, tf.float32)))\n",
    "             for t in (age_logits, gen_logits, skin_logits)]\n",
    "    return alpha * tf.add_n(terms)\n",
    "\n",
    "def calculate_total_loss(losses: dict, loss_weights: dict) -> tf.Tensor:\n",
    "    total = tf.constant(0.0, dtype=tf.float32)\n",
    "    for k, v in losses.items():\n",
    "        w = loss_weights.get(k, 0.0)\n",
    "        total += tf.cast(w, tf.float32) * tf.cast(v, tf.float32)\n",
    "    return total\n",
    "\n",
    "# ==================== FAIRNESS CVaR LOSS ========================\n",
    "def calculate_fairness_loss(reconstruction_losses_fake,\n",
    "                            age_labels,\n",
    "                            gender_labels,\n",
    "                            skin_tone_labels,\n",
    "                            cvar_alpha=0.2):\n",
    "    total_cvar_loss = tf.constant(0.0, dtype=tf.float32)\n",
    "    num_groups = tf.constant(0, dtype=tf.int32)\n",
    "\n",
    "    age_indices = tf.argmax(age_labels, axis=-1, output_type=tf.int32)\n",
    "    gender_indices = tf.argmax(gender_labels, axis=-1, output_type=tf.int32)\n",
    "    skin_indices = tf.argmax(skin_tone_labels, axis=-1, output_type=tf.int32)\n",
    "\n",
    "    n_age = tf.shape(age_labels, out_type=tf.int32)[1]\n",
    "    n_gender = tf.shape(gender_labels, out_type=tf.int32)[1]\n",
    "    n_skin = tf.shape(skin_tone_labels, out_type=tf.int32)[1]\n",
    "\n",
    "    demographic = {\n",
    "        'age': (age_indices, n_age),\n",
    "        'gender': (gender_indices, n_gender),\n",
    "        'skin': (skin_indices, n_skin),\n",
    "    }\n",
    "\n",
    "    for _, (indices, n_classes) in demographic.items():\n",
    "        for gid in tf.range(n_classes, dtype=tf.int32):\n",
    "            mask = tf.equal(indices, gid)\n",
    "            mask_any = tf.reduce_any(mask)\n",
    "            if not mask_any:\n",
    "                continue\n",
    "            selected = tf.boolean_mask(reconstruction_losses_fake, mask)\n",
    "            sorted_sel = tf.sort(selected, direction='DESCENDING')\n",
    "            n = tf.shape(sorted_sel, out_type=tf.int32)[0]\n",
    "            k = tf.cast(tf.cast(n, tf.float32) * cvar_alpha, tf.int32)\n",
    "            k = tf.maximum(k, 1)\n",
    "            top_k = sorted_sel[:k]\n",
    "            cvar = tf.reduce_mean(top_k)\n",
    "            total_cvar_loss += tf.cast(cvar, tf.float32)\n",
    "            num_groups += 1\n",
    "\n",
    "    total_cvar_loss = tf.math.divide_no_nan(total_cvar_loss, tf.cast(num_groups, tf.float32))\n",
    "    return {'fairness_cvar_loss': total_cvar_loss}\n",
    "\n",
    "# ==================== LOSS WEIGHTS ========================\n",
    "LOSS_WEIGHTS_BASE = {\n",
    "    'reconstruction_loss': 1.0,\n",
    "    'latent_loss': 1.0,\n",
    "    'age_adversarial_loss': 0.1,\n",
    "    'gender_adversarial_loss': 0.1,\n",
    "    'skin_tone_adversarial_loss': 0.1,\n",
    "    'fairness_cvar_loss': 0.5,\n",
    "}\n",
    "\n",
    "# ================== DATA HELPERS ====================\n",
    "def _norm(s: str) -> str:\n",
    "    return re.sub(r'[^a-z0-9]+', '', str(s).strip().lower())\n",
    "\n",
    "def resolve_csv_path(rp: str) -> str:\n",
    "    rp = str(rp).strip().replace(\"\\\\\",\"/\")\n",
    "    if os.path.isabs(rp): return os.path.normpath(rp)\n",
    "    if rp.startswith(PREPROC_DIR + \"/\") or rp.startswith(PREPROC_DIR + \"\\\\\"):\n",
    "        return os.path.normpath(os.path.join(DATASET_ROOT, rp))\n",
    "    if rp.startswith((\"train/\", \"val/\", \"test/\", \"./train/\", \"./val/\")):\n",
    "        return os.path.normpath(os.path.join(DATASET_ROOT, PREPROC_DIR, rp))\n",
    "    return os.path.normpath(os.path.join(IMAGE_ROOT, rp))\n",
    "\n",
    "def load_labels(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    need = [CSV_REL_PATH_COL, CSV_LABEL_COL, CSV_AGE_COL, CSV_GENDER_COL, CSV_SKIN_COL]\n",
    "    for col in need:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"CSV missing required column: {col}\")\n",
    "    LABEL_MAP = {'0':0,'1':1,'fake':0,'real':1}\n",
    "    AGE_MAP   = {'0':0,'1':1,'2':2,'young':0,'adult':1,'middleage':1,'middle-aged':1,'senior':2,'old':2}\n",
    "    GENDER_MAP= {'0':0,'1':1,'f':0,'female':0,'m':1,'male':1}\n",
    "    SKIN_MAP  = {'0':0,'1':1,'2':2,'3':3,'verylight':0,'pale':0,'light':1,'fair':1,'medium':2,'olive':2,'tan':2,\n",
    "                 'dark':3,'verydark':3,'brown':3,'black':3}\n",
    "    def map_or_die(series, name, mapping, allowed):\n",
    "        vals = series.astype(str)\n",
    "        mapped = vals.map(lambda v: mapping.get(_norm(v), v))\n",
    "        mapped = mapped.astype(int)\n",
    "        bad = sorted(set(int(x) for x in mapped.unique() if int(x) not in allowed))\n",
    "        if bad: raise ValueError(f\"{name}: values {bad} not in allowed set {allowed}\")\n",
    "        return mapped\n",
    "    df[CSV_LABEL_COL]  = map_or_die(df[CSV_LABEL_COL],  \"label\",     LABEL_MAP, {0,1})\n",
    "    df[CSV_AGE_COL]    = map_or_die(df[CSV_AGE_COL],    \"age_group\", AGE_MAP,   set(range(AGE_NUM_CLASSES)))\n",
    "    df[CSV_GENDER_COL] = map_or_die(df[CSV_GENDER_COL], \"gender\",    GENDER_MAP,set(range(GENDER_NUM_CLASSES)))\n",
    "    df[CSV_SKIN_COL]   = map_or_die(df[CSV_SKIN_COL],   \"skin_tone\", SKIN_MAP,  set(range(SKIN_NUM_CLASSES)))\n",
    "    df['abspath'] = df[CSV_REL_PATH_COL].map(resolve_csv_path)\n",
    "    exists_mask = df['abspath'].apply(os.path.exists)\n",
    "    missing_count = int((~exists_mask).sum())\n",
    "    if missing_count:\n",
    "        print(f\"Warning: {missing_count} files listed in CSV not found on disk — they will be skipped.\")\n",
    "        print(df.loc[~exists_mask, CSV_REL_PATH_COL].head(10).to_list())\n",
    "    df = df[exists_mask].copy().reset_index(drop=True)\n",
    "    df = df.rename(columns={\n",
    "        CSV_LABEL_COL: \"is_real\",\n",
    "        CSV_AGE_COL:   \"age_group\",\n",
    "        CSV_GENDER_COL:\"gender\",\n",
    "        CSV_SKIN_COL:  \"skin_tone\",\n",
    "    })\n",
    "    return df[[\"abspath\",\"is_real\",\"age_group\",\"gender\",\"skin_tone\"]]\n",
    "\n",
    "# --- ADD: compute class weight for imbalanced real/fake ---\n",
    "def compute_real_fake_pos_weight(df):\n",
    "    counts = df[\"is_real\"].value_counts().to_dict()\n",
    "    num_fake = float(counts.get(0, 1.0))\n",
    "    num_real = float(counts.get(1, 1.0))\n",
    "    return max(num_fake / max(num_real, 1.0), 1.0)\n",
    "\n",
    "# ==================== TF.DATA PIPELINE ====================\n",
    "def decode_image_no_crash(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=1)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    return img\n",
    "\n",
    "def process_sample(path, is_real, age, gender, skin):\n",
    "    img = decode_image_no_crash(path)\n",
    "    age_oh = tf.one_hot(tf.cast(age, tf.int32), AGE_NUM_CLASSES)\n",
    "    gen_oh = tf.one_hot(tf.cast(gender, tf.int32), GENDER_NUM_CLASSES)\n",
    "    skin_oh = tf.one_hot(tf.cast(skin, tf.int32), SKIN_NUM_CLASSES)\n",
    "    return img, (tf.cast(is_real, tf.int32),\n",
    "                 tf.cast(age_oh, tf.float32),\n",
    "                 tf.cast(gen_oh, tf.float32),\n",
    "                 tf.cast(skin_oh, tf.float32))\n",
    "\n",
    "def make_dataset(df, training=True, cache_dir=None):\n",
    "    paths = df['abspath'].values.astype(str)\n",
    "    is_reals = df['is_real'].values.astype(np.int32)\n",
    "    ages  = df['age_group'].values.astype(np.int32)\n",
    "    gens  = df['gender'].values.astype(np.int32)\n",
    "    skins = df['skin_tone'].values.astype(np.int32)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, is_reals, ages, gens, skins))\n",
    "    ds = ds.map(lambda p,ir,a,g,s: process_sample(p,ir,a,g,s), num_parallel_calls=AUTOTUNE)\n",
    "    if training:\n",
    "        ds = ds.shuffle(min(len(df), MAX_SHUFFLE_BUFFER), reshuffle_each_iteration=True)\n",
    "    if cache_dir:\n",
    "        os.makedirs(cache_dir, exist_ok=True); ds = ds.cache(cache_dir)\n",
    "    ds = ds.batch(GLOBAL_BATCH_SIZE, drop_remainder=True).prefetch(PREFETCH_BUFSIZE)\n",
    "    return ds\n",
    "\n",
    "# ==================== FEATURE EXTRACTION & BINARY HEAD ================\n",
    "def extract_features(dataset, encoder_model, latent_dim, frft_precomp):\n",
    "    X_list, y_list = [], []\n",
    "    for batch in dataset:\n",
    "        imgs, labels = batch\n",
    "        is_real_batch = labels[0]\n",
    "        imgs_frft = frft_precomp.frft2_grid_mag01(imgs)\n",
    "        z = encoder_model(imgs_frft, training=False)\n",
    "        X_list.append(tf.cast(z, tf.float32).numpy())\n",
    "        y_list.append(tf.cast(is_real_batch, tf.int32).numpy())\n",
    "    if not X_list:\n",
    "        return np.zeros((0, latent_dim), dtype=np.float32), np.zeros((0,), dtype=np.int32)\n",
    "    return np.concatenate(X_list, axis=0), np.concatenate(y_list, axis=0).astype(np.int32)\n",
    "\n",
    "def build_binary_head(latent_dim, leaky_alpha=0.1):\n",
    "    inp = Input(shape=(latent_dim,), dtype='float32')\n",
    "    x = LayerNormalization(epsilon=1e-5)(inp)\n",
    "    x = Dense(max(16, latent_dim//2), activation=None, kernel_initializer='he_normal')(x); x = LeakyReLU(leaky_alpha)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(max(8, latent_dim//4), activation=None, kernel_initializer='he_normal')(x); x = LeakyReLU(leaky_alpha)(x)\n",
    "    out = Dense(1, activation='sigmoid', dtype='float32')(x)\n",
    "    model = Model(inp, out, name='binary_head')\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "                  loss=BinaryCrossentropy(),\n",
    "                  metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "    return model\n",
    "\n",
    "# ==================== TRAIN/VAL STEPS ======================\n",
    "def dist_train_step(batch, grl_lambda, loss_weights):\n",
    "    imgs, labels = batch\n",
    "    age_lab, gender_lab, skin_lab = labels[1], labels[2], labels[3]\n",
    "\n",
    "    # Build FRFT inputs\n",
    "    imgs_frft = frft_pre.frft2_grid_mag01(imgs)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Encode FRFT magnitude\n",
    "        z = encoder(imgs_frft, training=True)\n",
    "\n",
    "        # Decode spatial image\n",
    "        recon_img = decoder(z, training=True)\n",
    "\n",
    "        # Re-encode FRFT of reconstructed image for latent cycle\n",
    "        recon_frft = frft_pre.frft2_grid_mag01(recon_img)\n",
    "        z_hat = encoder(recon_frft, training=True)\n",
    "\n",
    "        # Adversarial heads with GRL\n",
    "        z_rev = grad_reverse(z, grl_lambda)\n",
    "        age_pred, gen_pred, skin_pred = classifier(z_rev, training=True)  # logits\n",
    "\n",
    "        # Reconstruction losses: image-space + FRFT-space\n",
    "        per_ex_recon_img  = tf.reduce_mean(tf.square(tf.cast(imgs, tf.float32)     - tf.cast(recon_img, tf.float32)), axis=[1,2,3])\n",
    "        per_ex_recon_frft = tf.reduce_mean(tf.square(tf.cast(imgs_frft, tf.float32) - tf.cast(recon_frft, tf.float32)), axis=[1,2,3])\n",
    "        recon_loss = RECON_IMG_WEIGHT * tf.reduce_mean(per_ex_recon_img) + RECON_FRFT_WEIGHT * tf.reduce_mean(per_ex_recon_frft)\n",
    "\n",
    "        # Latent cycle loss\n",
    "        lat_loss = latent_loss(z, z_hat, beta=1.0)\n",
    "\n",
    "        # Adversarial classification losses\n",
    "        age_ce = ce(age_lab, tf.cast(age_pred, tf.float32))\n",
    "        gen_ce = ce(gender_lab, tf.cast(gen_pred, tf.float32))\n",
    "        skin_ce = ce(skin_lab, tf.cast(skin_pred, tf.float32))\n",
    "\n",
    "        logit_pen = logit_norm_penalty(age_pred, gen_pred, skin_pred, alpha=1e-4)\n",
    "\n",
    "        # Fairness CVaR computed on FRFT reconstruction errors (per-sample)\n",
    "        fairness_losses = calculate_fairness_loss(per_ex_recon_frft, age_lab, gender_lab, skin_lab, cvar_alpha=0.2)\n",
    "        fairness_loss = fairness_losses['fairness_cvar_loss']\n",
    "\n",
    "        # real/fake head\n",
    "        is_real = tf.cast(labels[0], tf.float32)\n",
    "        rf_logit = realfake_head(z, training=True)\n",
    "        rf_logit = tf.squeeze(tf.cast(rf_logit, tf.float32), axis=-1)\n",
    "        rf_loss_per = tf.nn.weighted_cross_entropy_with_logits(labels=is_real, logits=rf_logit, pos_weight=POS_WEIGHT_TF)\n",
    "        real_fake_loss = tf.reduce_mean(rf_loss_per)\n",
    "\n",
    "        # fairness warmup\n",
    "        fairness_scale = tf.minimum(1.0, CURRENT_EPOCH_TF / tf.cast(FAIRNESS_WARMUP_EPOCHS, tf.float32))\n",
    "\n",
    "        total = calculate_total_loss({\n",
    "            'reconstruction_loss': recon_loss,\n",
    "            'latent_loss': lat_loss,\n",
    "            'age_adversarial_loss': age_ce,\n",
    "            'gender_adversarial_loss': gen_ce,\n",
    "            'skin_tone_adversarial_loss': skin_ce,\n",
    "            'fairness_cvar_loss': fairness_loss,\n",
    "        }, loss_weights) + logit_pen\n",
    "\n",
    "        total += (fairness_scale - 1.0) * tf.cast(LOSS_WEIGHTS_BASE['fairness_cvar_loss'], tf.float32) * tf.cast(fairness_loss, tf.float32)\n",
    "        total += tf.cast(REALFAKE_LOSS_WEIGHT, tf.float32) * tf.cast(real_fake_loss, tf.float32)\n",
    "\n",
    "    vars_all = encoder.trainable_variables + decoder.trainable_variables + classifier.trainable_variables + realfake_head.trainable_variables\n",
    "    grads = tape.gradient(total, vars_all)\n",
    "    grads, _ = tf.clip_by_global_norm(grads, 5.0)\n",
    "    opt.apply_gradients(zip(grads, vars_all))\n",
    "\n",
    "    return recon_loss, lat_loss, age_ce, gen_ce, skin_ce, fairness_loss, total\n",
    "\n",
    "def dist_val_step(batch, grl_lambda, loss_weights):\n",
    "    imgs, labels = batch\n",
    "    age_lab, gender_lab, skin_lab = labels[1], labels[2], labels[3]\n",
    "    imgs_frft = frft_pre.frft2_grid_mag01(imgs)\n",
    "\n",
    "    z = encoder(imgs_frft, training=False)\n",
    "    recon_img = decoder(z, training=False)\n",
    "    recon_frft = frft_pre.frft2_grid_mag01(recon_img)\n",
    "    z_hat = encoder(recon_frft, training=False)\n",
    "\n",
    "    z_rev = grad_reverse(z, grl_lambda)\n",
    "    age_pred, gen_pred, skin_pred = classifier(z_rev, training=False)  # logits\n",
    "\n",
    "    per_ex_recon_img  = tf.reduce_mean(tf.square(tf.cast(imgs, tf.float32)     - tf.cast(recon_img, tf.float32)), axis=[1,2,3])\n",
    "    per_ex_recon_frft = tf.reduce_mean(tf.square(tf.cast(imgs_frft, tf.float32) - tf.cast(recon_frft, tf.float32)), axis=[1,2,3])\n",
    "    recon_loss = RECON_IMG_WEIGHT * tf.reduce_mean(per_ex_recon_img) + RECON_FRFT_WEIGHT * tf.reduce_mean(per_ex_recon_frft)\n",
    "\n",
    "    lat_loss = latent_loss(z, z_hat, beta=1.0)\n",
    "    age_ce = ce(age_lab, tf.cast(age_pred, tf.float32))\n",
    "    gen_ce = ce(gender_lab, tf.cast(gen_pred, tf.float32))\n",
    "    skin_ce = ce(skin_lab, tf.cast(skin_pred, tf.float32))\n",
    "\n",
    "    fairness_losses = calculate_fairness_loss(per_ex_recon_frft, age_lab, gender_lab, skin_lab, cvar_alpha=0.2)\n",
    "    fairness_loss = fairness_losses['fairness_cvar_loss']\n",
    "\n",
    "    is_real = tf.cast(labels[0], tf.float32)\n",
    "    rf_logit = realfake_head(z, training=False)\n",
    "    rf_logit = tf.squeeze(tf.cast(rf_logit, tf.float32), axis=-1)\n",
    "    rf_loss_per = tf.nn.weighted_cross_entropy_with_logits(labels=is_real, logits=rf_logit, pos_weight=POS_WEIGHT_TF)\n",
    "    real_fake_loss = tf.reduce_mean(rf_loss_per)\n",
    "\n",
    "    fairness_scale = tf.minimum(1.0, CURRENT_EPOCH_TF / tf.cast(FAIRNESS_WARMUP_EPOCHS, tf.float32))\n",
    "\n",
    "    total = calculate_total_loss({\n",
    "            'reconstruction_loss': recon_loss,\n",
    "            'latent_loss': lat_loss,\n",
    "            'age_adversarial_loss': age_ce,\n",
    "            'gender_adversarial_loss': gen_ce,\n",
    "            'skin_tone_adversarial_loss': skin_ce,\n",
    "            'fairness_cvar_loss': fairness_loss,\n",
    "    }, loss_weights) + logit_norm_penalty(age_pred, gen_pred, skin_pred, alpha=1e-4)\n",
    "\n",
    "    total += (fairness_scale - 1.0) * tf.cast(LOSS_WEIGHTS_BASE['fairness_cvar_loss'], tf.float32) * tf.cast(fairness_loss, tf.float32)\n",
    "    total += tf.cast(REALFAKE_LOSS_WEIGHT, tf.float32) * tf.cast(real_fake_loss, tf.float32)\n",
    "\n",
    "    return recon_loss, lat_loss, age_ce, gen_ce, skin_ce, fairness_loss, total\n",
    "\n",
    "# ================== MAIN ===========================\n",
    "def main():\n",
    "    global encoder, decoder, classifier, opt, frft_pre, ce, realfake_head, POS_WEIGHT_TF\n",
    "\n",
    "    tf.random.set_seed(42); np.random.seed(42)\n",
    "    H, W = IMG_SIZE\n",
    "\n",
    "    ALPHA_GRID_RAD = [float(np.deg2rad(a)) for a in [0,30,60,90]]\n",
    "    frft_pre = FRFTPrecomp(H, W, ALPHA_GRID_RAD, ALPHA_GRID_RAD)\n",
    "\n",
    "    df = load_labels(CSV_PATH)\n",
    "    if df.empty:\n",
    "        raise RuntimeError(\"No valid image files found after resolving CSV paths.\")\n",
    "    print(f\"Found {len(df)} valid files after resolving paths.\")\n",
    "\n",
    "    val_n = int(len(df) * VAL_SPLIT)\n",
    "    val_df = df.iloc[:val_n].reset_index(drop=True)\n",
    "    train_df = df.iloc[val_n:].reset_index(drop=True)\n",
    "    print(f\"Train / Val sizes: {len(train_df)} / {len(val_df)}\")\n",
    "\n",
    "    tr_ds = make_dataset(train_df, training=True, cache_dir=None)\n",
    "    va_ds = make_dataset(val_df, training=False, cache_dir=None)\n",
    "\n",
    "    tr_dist = strategy.experimental_distribute_dataset(tr_ds)\n",
    "    va_dist = strategy.experimental_distribute_dataset(va_ds)\n",
    "\n",
    "    with strategy.scope():\n",
    "        encoder, decoder = build_shared_autoencoder((H, W, 1), LATENT_DIM)\n",
    "        classifier = build_classifier(LATENT_DIM)\n",
    "        realfake_head = build_realfake_head(LATENT_DIM)\n",
    "        opt = AdamW(learning_rate=LEARNING_RATE, weight_decay=1e-4)\n",
    "        ce  = CategoricalCrossentropy(from_logits=True, label_smoothing=0.05)\n",
    "\n",
    "    # --- class weight for weighted BCE ---\n",
    "    pos_weight_scalar = compute_real_fake_pos_weight(train_df)\n",
    "    POS_WEIGHT_TF = tf.constant(pos_weight_scalar, dtype=tf.float32)\n",
    "    print(f\"Real/Fake pos_weight = {pos_weight_scalar:.3f}\")\n",
    "\n",
    "    # --- early stopping bookkeeping ---\n",
    "    os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "    best_val_recon = np.inf\n",
    "    best_val_fair  = np.inf\n",
    "    es_wait = 0\n",
    "    # training loop\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        t0 = time.time()\n",
    "        CURRENT_EPOCH_TF.assign(float(epoch))\n",
    "        grl_lambda = tf.constant(min(0.5, epoch / 10.0), tf.float32)\n",
    "\n",
    "        tr_sums = np.zeros(7, dtype=np.float64); tr_batches = 0\n",
    "        for batch in tr_dist:\n",
    "            per_replica_outs = strategy.run(dist_train_step, args=(batch, grl_lambda, LOSS_WEIGHTS_BASE))\n",
    "            outs_means = []\n",
    "            for i in range(7):\n",
    "                local_vals = strategy.experimental_local_results(per_replica_outs[i])\n",
    "                outs_means.append(float(tf.reduce_mean(tf.stack(local_vals)).numpy()))\n",
    "            tr_sums += np.array(outs_means); tr_batches += 1\n",
    "\n",
    "        va_sums = np.zeros(7, dtype=np.float64); va_batches = 0\n",
    "        for batch in va_dist:\n",
    "            per_replica_outs = strategy.run(dist_val_step, args=(batch, grl_lambda, LOSS_WEIGHTS_BASE))\n",
    "            outs_means = []\n",
    "            for i in range(7):\n",
    "                local_vals = strategy.experimental_local_results(per_replica_outs[i])\n",
    "                outs_means.append(float(tf.reduce_mean(tf.stack(local_vals)).numpy()))\n",
    "            va_sums += np.array(outs_means); va_batches += 1\n",
    "\n",
    "        tr_avg = tr_sums / max(1, tr_batches)\n",
    "        va_avg = va_sums / max(1, va_batches)\n",
    "        t1 = time.time()\n",
    "\n",
    "        print(f\"[Epoch {epoch:02d}/{EPOCHS}] time/epoch={t1-t0:.1f}s\")\n",
    "        print(f\"  Train: recon={tr_avg[0]:.4f} lat={tr_avg[1]:.4f} age={tr_avg[2]:.4f} gen={tr_avg[3]:.4f} skin={tr_avg[4]:.4f} fairness={tr_avg[5]:.6f} total={tr_avg[6]:.4f}\")\n",
    "        print(f\"  Val  : recon={va_avg[0]:.4f} lat={va_avg[1]:.4f} age={va_avg[2]:.4f} gen={va_avg[3]:.4f} skin={va_avg[4]:.4f} fairness={va_avg[5]:.6f} total={va_avg[6]:.4f}\")\n",
    "\n",
    "           # ===========================================================\n",
    "        # 🔍 FAIRNESS-AWARE EARLY STOPPING (reconstruction + fairness)\n",
    "        # ===========================================================\n",
    "        # ===========================================================\n",
    "        # 🔍 FAIRNESS + RECONSTRUCTION EARLY STOPPING\n",
    "        # ===========================================================\n",
    "        val_recon = va_avg[0]  # reconstruction loss (image + FRFT)\n",
    "        val_fair  = va_avg[5]  # fairness CVaR loss\n",
    "        \n",
    "        # Initialize tracking variables before the epoch loop if not already\n",
    "        # best_val_recon = np.inf\n",
    "        # best_val_fair  = np.inf\n",
    "        # es_wait = 0\n",
    "        \n",
    "        # Criteria for improvement: either recon or fairness improved\n",
    "        RECON_TOLERANCE = 0.995  # allow slight fluctuation\n",
    "        FAIRNESS_TOLERANCE = 0.995\n",
    "        \n",
    "        improved_recon = val_recon < best_val_recon * RECON_TOLERANCE\n",
    "        improved_fair  = val_fair  < best_val_fair * FAIRNESS_TOLERANCE\n",
    "        \n",
    "        if improved_recon or improved_fair:\n",
    "            best_val_recon = min(best_val_recon, val_recon)\n",
    "            best_val_fair  = min(best_val_fair, val_fair)\n",
    "            es_wait = 0\n",
    "        \n",
    "            # Save best weights\n",
    "            encoder.save_weights(os.path.join(CKPT_DIR, \"encoder.best.weights.h5\"))\n",
    "            decoder.save_weights(os.path.join(CKPT_DIR, \"decoder.best.weights.h5\"))\n",
    "            classifier.save_weights(os.path.join(CKPT_DIR, \"classifier.best.weights.h5\"))\n",
    "            realfake_head.save_weights(os.path.join(CKPT_DIR, \"realfake_head.best.weights.h5\"))\n",
    "        \n",
    "            print(f\"  ✅ Improvement detected — Recon: {val_recon:.6f}, Fair: {val_fair:.6f} — checkpointed.\")\n",
    "        \n",
    "        else:\n",
    "            es_wait += 1\n",
    "            print(f\"  ⚠️  No improvement ({es_wait}/{ES_PATIENCE}) — Recon={val_recon:.6f}, Fair={val_fair:.6f}\")\n",
    "            \n",
    "            if es_wait >= ES_PATIENCE:\n",
    "                print(\"  ⛔ Early stopping triggered — recon + fairness plateau reached.\")\n",
    "                break\n",
    "\n",
    "    try:\n",
    "        encoder.load_weights(os.path.join(CKPT_DIR, \"encoder.best.weights.h5\"))\n",
    "        decoder.load_weights(os.path.join(CKPT_DIR, \"decoder.best.weights.h5\"))\n",
    "        classifier.load_weights(os.path.join(CKPT_DIR, \"classifier.best.weights.h5\"))\n",
    "        realfake_head.load_weights(os.path.join(CKPT_DIR, \"realfake_head.best.weights.h5\"))\n",
    "        print(\"Loaded best checkpointed weights.\")\n",
    "    except Exception as e:\n",
    "        print(\"Warning: failed to load best weights (continuing with current):\", e)\n",
    "\n",
    "    # ---- Feature extraction & binary head ----\n",
    "    print(\"Extracting features for binary head training...\")\n",
    "    X_tr, y_tr = extract_features(tr_ds, encoder, LATENT_DIM, frft_pre)\n",
    "    X_va, y_va = extract_features(va_ds, encoder, LATENT_DIM, frft_pre)\n",
    "    print(f\"Features shapes: X_tr {X_tr.shape}, y_tr {y_tr.shape} | X_va {X_va.shape}, y_va {y_va.shape}\")\n",
    "\n",
    "    bin_head = build_binary_head(LATENT_DIM)\n",
    "\n",
    "    if X_tr.shape[0] > 0:\n",
    "        bin_head.fit(\n",
    "            X_tr, y_tr.astype(np.float32),\n",
    "            validation_data=(X_va, y_va.astype(np.float32)),\n",
    "            epochs=50, batch_size=512,\n",
    "            callbacks=[\n",
    "                tf.keras.callbacks.EarlyStopping(monitor='val_auc', mode='max', patience=7, restore_best_weights=True),\n",
    "                tf.keras.callbacks.ReduceLROnPlateau(monitor='val_auc', mode='max', factor=0.5, patience=3, verbose=1),\n",
    "                tf.keras.callbacks.ModelCheckpoint(\n",
    "                    filepath=os.path.join(CKPT_DIR, \"binary_head.best.weights.h5\"),\n",
    "                    monitor='val_auc', mode='max', save_best_only=True, save_weights_only=True\n",
    "                )\n",
    "            ],\n",
    "            verbose=1\n",
    "        )\n",
    "        # load best binary head\n",
    "        try:\n",
    "            bin_head.load_weights(os.path.join(CKPT_DIR, \"binary_head.best.weights.h5\"))\n",
    "            print(\"Loaded best binary head weights.\")\n",
    "        except Exception as e:\n",
    "            print(\"Warning loading best binary head:\", e)\n",
    "    else:\n",
    "        print(\"No training examples found for binary head; skipping bin_head.fit\")\n",
    "\n",
    "    # save final weights (optional)\n",
    "    os.makedirs(FINAL_SAVE_DIR, exist_ok=True)\n",
    "    try:\n",
    "        encoder.save_weights(os.path.join(FINAL_SAVE_DIR, \"encoder.weights.h5\"))\n",
    "        decoder.save_weights(os.path.join(FINAL_SAVE_DIR, \"decoder.weights.h5\"))\n",
    "        classifier.save_weights(os.path.join(FINAL_SAVE_DIR, \"classifier.weights.h5\"))\n",
    "        realfake_head.save_weights(os.path.join(FINAL_SAVE_DIR, \"realfake_head.weights.h5\"))\n",
    "        bin_head.save_weights(os.path.join(FINAL_SAVE_DIR, \"binary_head.weights.h5\"))\n",
    "        print(\"Saved final model weights to\", FINAL_SAVE_DIR)\n",
    "    except Exception as e:\n",
    "        print(\"Warning: failed to save some weights:\", e)\n",
    "\n",
    "    return encoder, decoder, classifier, bin_head, frft_pre\n",
    "\n",
    "def plot_fairness(y_true, y_pred_prob, sensitive_attr, groups, WEIGHTS_DIR):\n",
    "    \"\"\"\n",
    "    Plots fairness metrics: CVaR, Equalized Odds, demographic parity gap.\n",
    "    \"\"\"\n",
    "    # CVaR (tail of losses) per group\n",
    "    cvar_dict = {}\n",
    "    alpha = 0.9  # top 10% losses\n",
    "    losses = -(y_true * np.log(y_pred_prob + 1e-12) + (1 - y_true) * np.log(1 - y_pred_prob + 1e-12))\n",
    "    \n",
    "    for g in groups:\n",
    "        g_mask = (sensitive_attr == g)\n",
    "        if g_mask.sum() == 0: continue\n",
    "        g_losses = losses[g_mask]\n",
    "        threshold = np.quantile(g_losses, alpha)\n",
    "        cvar = g_losses[g_losses >= threshold].mean()\n",
    "        cvar_dict[g] = cvar\n",
    "    \n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.barplot(x=list(cvar_dict.keys()), y=list(cvar_dict.values()))\n",
    "    plt.title(f\"CVaR @ {alpha*100:.0f}% Losses per Group\")\n",
    "    plt.ylabel(\"CVaR Loss\"); plt.xlabel(\"Group\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(WEIGHTS_DIR, \"fairness_cvar.png\"), dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    # Equalized Odds (TPR/FPR per group)\n",
    "    eo_dict = {}\n",
    "    for g in groups:\n",
    "        g_mask = (sensitive_attr == g)\n",
    "        if g_mask.sum() == 0: continue\n",
    "        y_g = y_true[g_mask]\n",
    "        y_pred_g = (y_pred_prob[g_mask] >= 0.5).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_g, y_pred_g, labels=[0,1]).ravel()\n",
    "        tpr = tp / (tp + fn + 1e-12)\n",
    "        fpr = fp / (fp + tn + 1e-12)\n",
    "        eo_dict[g] = (tpr, fpr)\n",
    "\n",
    "    tpr_vals = [v[0] for v in eo_dict.values()]\n",
    "    fpr_vals = [v[1] for v in eo_dict.values()]\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.barplot(x=list(eo_dict.keys()), y=tpr_vals, alpha=0.7, label=\"TPR\")\n",
    "    sns.barplot(x=list(eo_dict.keys()), y=fpr_vals, alpha=0.7, label=\"FPR\")\n",
    "    plt.ylabel(\"Rate\"); plt.xlabel(\"Group\")\n",
    "    plt.title(\"Equalized Odds: TPR / FPR per Group\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(WEIGHTS_DIR, \"fairness_equalized_odds.png\"), dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    # Demographic Parity\n",
    "    dp_dict = {}\n",
    "    for g in groups:\n",
    "        g_mask = (sensitive_attr == g)\n",
    "        if g_mask.sum() == 0: continue\n",
    "        dp_dict[g] = y_pred_prob[g_mask].mean()\n",
    "    \n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.barplot(x=list(dp_dict.keys()), y=list(dp_dict.values()))\n",
    "    plt.title(\"Demographic Parity (mean predicted probability) per group\")\n",
    "    plt.ylabel(\"Mean predicted probability\")\n",
    "    plt.xlabel(\"Group\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(WEIGHTS_DIR, \"fairness_demographic_parity.png\"), dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ================== EVAL ON TEST ==================\n",
    "def eval_on_test(encoder_model, bin_head_model, frft_precomp, use_frft=True):\n",
    "    WEIGHTS_DIR = FINAL_SAVE_DIR\n",
    "    TEST_IMAGE_ROOT = f\"{DATASET_ROOT}/{PREPROC_DIR}/test\"\n",
    "\n",
    "    global IMAGE_ROOT\n",
    "    prev_root = IMAGE_ROOT\n",
    "    IMAGE_ROOT = TEST_IMAGE_ROOT\n",
    "    try:\n",
    "        df_test = load_labels(CSV_PATH)\n",
    "    finally:\n",
    "        IMAGE_ROOT = prev_root\n",
    "\n",
    "    print(f\"Test rows: {len(df_test)} | real(1)/fake(0): {df_test['is_real'].value_counts().to_dict()}\")\n",
    "\n",
    "    def _decode_resize(path):\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.image.decode_jpeg(img, channels=1)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        img = tf.image.resize(img, IMG_SIZE)\n",
    "        return img\n",
    "\n",
    "    def _apply_frft_py(img, frft_obj):\n",
    "        return frft_obj.frft2_grid_mag01(tf.expand_dims(img, 0))[0]\n",
    "\n",
    "    def make_plain_ds_from_df(df_sub):\n",
    "        paths = df_sub[\"abspath\"].values.astype(str)\n",
    "        age   = df_sub[\"age_group\"].values.astype(np.int32)\n",
    "        gen   = df_sub[\"gender\"].values.astype(np.int32)\n",
    "        skin  = df_sub[\"skin_tone\"].values.astype(np.int32)\n",
    "        real  = df_sub[\"is_real\"].values.astype(np.int32)\n",
    "        ds = tf.data.Dataset.from_tensor_slices((paths, age, gen, skin, real))\n",
    "        ds = ds.map(lambda p,a,g,s,r: (_decode_resize(p), a, g, s, r), num_parallel_calls=AUTOTUNE)\n",
    "        if use_frft and frft_precomp is not None:\n",
    "            ds = ds.map(lambda x,a,g,s,r: (_apply_frft_py(x, frft_precomp), a,g,s,r), num_parallel_calls=AUTOTUNE)\n",
    "        ds = ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "        return ds\n",
    "\n",
    "    def extract_features_eval(ds):\n",
    "        feats, labels = [], []\n",
    "        for batch in ds:\n",
    "            x, *_, ri = batch\n",
    "            z = encoder_model(tf.cast(x, tf.float32), training=False)\n",
    "            feats.append(tf.cast(z, tf.float32).numpy())\n",
    "            labels.append(tf.cast(ri, tf.int32).numpy())\n",
    "        if not feats:\n",
    "            return np.zeros((0, LATENT_DIM), dtype=np.float32), np.zeros((0,), dtype=np.int32)\n",
    "        X = np.concatenate(feats, axis=0).astype(np.float32)\n",
    "        y = np.concatenate(labels, axis=0).astype(np.int32)\n",
    "        return X, y\n",
    "\n",
    "    test_ds = make_plain_ds_from_df(df_test)\n",
    "    X_te, y_te = extract_features_eval(test_ds)\n",
    "    if X_te.shape[0] == 0:\n",
    "        print(\"No test samples were found or extracted.\")\n",
    "        return\n",
    "\n",
    "    y_prob = bin_head_model.predict(X_te, batch_size=2048, verbose=1).ravel()\n",
    "\n",
    "    auc = roc_auc_score(y_te, y_prob)\n",
    "    ap  = average_precision_score(y_te, y_prob)\n",
    "    y_pred05 = (y_prob >= 0.5).astype(int)\n",
    "    acc05 = accuracy_score(y_te, y_pred05)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_te, y_pred05, labels=[0,1]).ravel()\n",
    "    fpr05 = fp / (fp + tn + 1e-12)\n",
    "\n",
    "    fpr, tpr, thr = roc_curve(y_te, y_prob)\n",
    "    fnr = 1.0 - tpr\n",
    "    idx = np.nanargmin(np.abs(fpr - fnr))\n",
    "    eer = max(fpr[idx], fnr[idx])\n",
    "    thr_eer = thr[idx]\n",
    "    y_pred_eer = (y_prob >= thr_eer).astype(int)\n",
    "    tn2, fp2, fn2, tp2 = confusion_matrix(y_te, y_pred_eer, labels=[0,1]).ravel()\n",
    "    fpr_eer = fp2 / (fp2 + tn2 + 1e-12)\n",
    "\n",
    "    print(f\"\\nAUC: {auc:.4f}\")\n",
    "    print(f\"AP : {ap:.4f}\")\n",
    "    print(f\"ACC @0.5: {acc05:.4f}\")\n",
    "    print(f\"FPR @0.5: {fpr05:.4f}\")\n",
    "    print(f\"EER: {eer:.4f} at threshold {thr_eer:.6f} (FPR at EER thr: {fpr_eer:.4f})\")\n",
    "\n",
    "    # ----- Plots -----\n",
    "    # ROC\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.plot(fpr, tpr, label=f'ROC (AUC={auc:.3f})')\n",
    "    plt.plot([0,1],[0,1], ls='--', lw=1)\n",
    "    plt.scatter([eer], [1-eer], s=30, label=f'EER={eer:.3f}')\n",
    "    plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve (Test)'); plt.legend(); plt.grid(True, alpha=0.3)\n",
    "    if SAVE_PLOTS:\n",
    "        os.makedirs(WEIGHTS_DIR, exist_ok=True)\n",
    "        plt.tight_layout(); plt.savefig(f\"{WEIGHTS_DIR}/test_roc_curve.png\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    # PR\n",
    "    prec, rec, _ = precision_recall_curve(y_te, y_prob)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.plot(rec, prec, label=f'AP={ap:.3f}')\n",
    "    plt.xlabel('Recall'); plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall (Test)'); plt.legend(); plt.grid(True, alpha=0.3)\n",
    "    if SAVE_PLOTS:\n",
    "        plt.tight_layout(); plt.savefig(f\"{WEIGHTS_DIR}/test_pr_curve.png\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    # Score hist\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.hist(y_prob[y_te==0], bins=50, alpha=0.6, label='fake (0)')\n",
    "    plt.hist(y_prob[y_te==1], bins=50, alpha=0.6, label='real (1)')\n",
    "    plt.axvline(0.5, ls='--', lw=1)\n",
    "    plt.axvline(thr_eer, ls=':', lw=1)\n",
    "    plt.xlabel('Predicted probability (real)'); plt.ylabel('Count')\n",
    "    plt.title('Score distribution (Test)'); plt.legend()\n",
    "    if SAVE_PLOTS:\n",
    "        plt.tight_layout(); plt.savefig(f\"{WEIGHTS_DIR}/test_score_hist.png\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    # Fairness plots\n",
    "    for attr in [\"gender\", \"skin_tone\"]:\n",
    "        plot_fairness(\n",
    "            y_true=y_te,\n",
    "            y_pred_prob=y_prob,\n",
    "            sensitive_attr=df_test[attr].values,\n",
    "            groups=np.unique(df_test[attr]),\n",
    "            WEIGHTS_DIR=WEIGHTS_DIR\n",
    "        )\n",
    "\n",
    "    # ========== FAIRNESS METRICS: FPR, MEO, DP, OAE ==========\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FAIRNESS METRICS (Lower = Better)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    fairness_results = {}\n",
    "    \n",
    "    for attr_name in [\"gender\", \"age_group\", \"skin_tone\"]:\n",
    "        print(f\"\\n--- Attribute: {attr_name.upper()} ---\")\n",
    "        sensitive = df_test[attr_name].values\n",
    "        groups = np.unique(sensitive)\n",
    "        \n",
    "        # Compute per-group metrics\n",
    "        fpr_dict = {}\n",
    "        tpr_dict = {}\n",
    "        acc_dict = {}\n",
    "        pred_rate_dict = {}\n",
    "        \n",
    "        for g in groups:\n",
    "            g_mask = (sensitive == g)\n",
    "            if g_mask.sum() == 0:\n",
    "                continue\n",
    "            \n",
    "            y_g = y_te[g_mask]\n",
    "            y_pred_g = y_pred05[g_mask]\n",
    "            y_prob_g = y_prob[g_mask]\n",
    "            \n",
    "            # Confusion matrix\n",
    "            tn_g, fp_g, fn_g, tp_g = confusion_matrix(y_g, y_pred_g, labels=[0,1]).ravel()\n",
    "            \n",
    "            # FPR, TPR, Accuracy\n",
    "            fpr_g = fp_g / (fp_g + tn_g + 1e-12)\n",
    "            tpr_g = tp_g / (tp_g + fn_g + 1e-12)\n",
    "            acc_g = (tp_g + tn_g) / (tp_g + tn_g + fp_g + fn_g + 1e-12)\n",
    "            pred_rate_g = y_pred_g.mean()  # Positive prediction rate\n",
    "            \n",
    "            fpr_dict[g] = fpr_g\n",
    "            tpr_dict[g] = tpr_g\n",
    "            acc_dict[g] = acc_g\n",
    "            pred_rate_dict[g] = pred_rate_g\n",
    "        \n",
    "        if len(fpr_dict) < 2:\n",
    "            print(f\"  Skipping (only {len(fpr_dict)} group(s) found)\")\n",
    "            continue\n",
    "        \n",
    "        # F_FPR: Fairness in False Positive Rate (max gap in FPR)\n",
    "        fpr_vals = list(fpr_dict.values())\n",
    "        f_fpr = max(fpr_vals) - min(fpr_vals)\n",
    "        \n",
    "        # F_MEO: Fairness in Mean Equalized Odds (avg disparity in TPR and FPR)\n",
    "        tpr_vals = list(tpr_dict.values())\n",
    "        tpr_gap = max(tpr_vals) - min(tpr_vals)\n",
    "        fpr_gap = max(fpr_vals) - min(fpr_vals)\n",
    "        f_meo = (tpr_gap + fpr_gap) / 2.0\n",
    "        \n",
    "        # F_DP: Fairness in Demographic Parity (gap in positive prediction rate)\n",
    "        pred_vals = list(pred_rate_dict.values())\n",
    "        f_dp = max(pred_vals) - min(pred_vals)\n",
    "        \n",
    "        # F_OAE: Fairness in Overall Accuracy Equality (gap in accuracy)\n",
    "        acc_vals = list(acc_dict.values())\n",
    "        f_oae = max(acc_vals) - min(acc_vals)\n",
    "        \n",
    "        print(f\"  F_FPR (FPR gap):        {f_fpr:.4f}\")\n",
    "        print(f\"  F_MEO (Eq. Odds gap):   {f_meo:.4f}\")\n",
    "        print(f\"  F_DP  (Pred. rate gap): {f_dp:.4f}\")\n",
    "        print(f\"  F_OAE (Accuracy gap):   {f_oae:.4f}\")\n",
    "        \n",
    "        # Store for CSV\n",
    "        fairness_results[f\"{attr_name}_F_FPR\"] = f_fpr\n",
    "        fairness_results[f\"{attr_name}_F_MEO\"] = f_meo\n",
    "        fairness_results[f\"{attr_name}_F_DP\"] = f_dp\n",
    "        fairness_results[f\"{attr_name}_F_OAE\"] = f_oae\n",
    "        \n",
    "        # Per-group breakdown\n",
    "        print(f\"\\n  Per-group breakdown:\")\n",
    "        for g in groups:\n",
    "            if g not in fpr_dict:\n",
    "                continue\n",
    "            print(f\"    Group {g}: FPR={fpr_dict[g]:.4f}, TPR={tpr_dict[g]:.4f}, Acc={acc_dict[g]:.4f}, PredRate={pred_rate_dict[g]:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # Save metrics & predictions\n",
    "    try:\n",
    "        os.makedirs(WEIGHTS_DIR, exist_ok=True)\n",
    "        # Merge standard and fairness metrics\n",
    "        all_metrics = {\n",
    "            \"auc\":[auc], \"ap\":[ap], \"acc@0.5\":[acc05],\n",
    "            \"fpr@0.5\":[fpr05], \"eer\":[eer], \"thr_eer\":[thr_eer], \"fpr@thr_eer\":[fpr_eer]\n",
    "        }\n",
    "        # Add fairness metrics\n",
    "        for k, v in fairness_results.items():\n",
    "            all_metrics[k] = [v]\n",
    "        \n",
    "        pd.DataFrame(all_metrics).to_csv(f\"{WEIGHTS_DIR}/test_metrics.csv\", index=False)\n",
    "\n",
    "        pred_df = pd.DataFrame({\"abspath\": df_test[\"abspath\"].values, \"label\": df_test[\"is_real\"].values, \"p_real\": y_prob})\n",
    "        pred_df.to_csv(f\"{WEIGHTS_DIR}/test_predictions.csv\", index=False)\n",
    "        print(\"\\nSaved test evaluation outputs in\", WEIGHTS_DIR)\n",
    "    except Exception as e:\n",
    "        print(\"Warning while saving metrics/predictions:\", e)\n",
    "\n",
    "# ================== RUN ====================\n",
    "if __name__ == \"__main__\":\n",
    "    encoder, decoder, classifier, bin_head, frft_pre = main()\n",
    "    eval_on_test(encoder, bin_head, frft_pre)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
